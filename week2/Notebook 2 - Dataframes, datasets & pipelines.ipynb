{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Machine Learning Library (MLlib)</h1>\n",
    "\n",
    "[MLlib](http://spark.apache.org/docs/latest/ml-guide.html) is Spark’s machine learning (ML) library. It provides:\n",
    "\n",
    "- *ML Algorithms*: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "- *Featurization*: feature extraction, transformation, dimensionality reduction, and selection\n",
    "- *Pipelines*: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "- *Persistence*: saving and load algorithms, models, and Pipelines\n",
    "- *Utilities*: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "We carry out the usual settings, classpath and imports, this time including <tt>MLlib</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sparkVersion = \"2.0.1\"\n",
    "val scalaVersion = scala.util.Properties.versionNumberString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classpath.add(\n",
    "    \"org.apache.spark\" %% \"spark-yarn\" % sparkVersion,\n",
    "    \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// imports for the text document pipeline\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Create Spark session\n",
    "val sparkSession = SparkSession.builder\n",
    "    .master(\"local[1]\")\n",
    "    .appName(\"Spark dataframes and datasets\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tt>MLlib</tt> allows easy combination of numerous algorithms into a single pipeline using standardized APIs for machine learning algorithms. The key concepts are:\n",
    "\n",
    "- **Dataframe**. Dataframes can hold a variety of data types.\n",
    "- **Transformer**. Transforms one dataframe into another.\n",
    "- **Estimator**. Algorithm which can be fit on a DataFrame to produce a Transformer.\n",
    "- **Pipeline**. A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "- **Parameter**. Transformers and Estimators share a common API for specifying parameters.\n",
    "\n",
    "More details on these below, and a list of some of the available ML features is available [here](http://spark.apache.org/docs/latest/ml-features.html).\n",
    "\n",
    "<h2>Datasets and Dataframes</h2>\n",
    "\n",
    "Along with the introduction of <tt>SparkSession</tt>, the <tt>resilient distributed dataset</tt> (RDD) was replaced by [dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset). Again, these are objects which can be worked on in parallel. The available operations are:\n",
    "\n",
    "- **transformations**: produce new datasets\n",
    "- **actions**: computations which return results\n",
    "\n",
    "We will start with creating dataframes and datasets, showing how we can print their contents. We create a dataframe in the cell below and print out some info (we can also modify the output before printing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// create a dataframe based on the contents of a JSON file\n",
    "val peopleDF = sparkSession.read.json(\"files/people.json\")\n",
    "\n",
    "peopleDF.show()\n",
    "\n",
    "// Print the schema in a tree format\n",
    "peopleDF.printSchema()\n",
    "\n",
    "// Select only the \"name\" column\n",
    "peopleDF.select(\"name\").show()\n",
    "\n",
    "// This import is needed to use the $-notation\n",
    "import sparkSession.implicits._\n",
    "\n",
    "// Select everybody, but increment the age by 1\n",
    "peopleDF.select($\"name\", $\"age\" + 1).show()\n",
    "\n",
    "// Select people older than 21\n",
    "peopleDF.filter($\"age\" > 21).show()\n",
    "\n",
    "// Count people by age\n",
    "peopleDF.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset example is in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// create a dataset using sparkSession.range starting from 5 to 100, with increments of 5\n",
    "val numDS = sparkSession.range(5, 100, 5)\n",
    "\n",
    "// order by column\n",
    "numDS.orderBy(\"id\").show(5)\n",
    "\n",
    "import sparkSession.implicits._\n",
    "\n",
    "numDS.orderBy($\"id\".desc).show(5)\n",
    "\n",
    "// compute descriptive stats and display them\n",
    "numDS.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dataframe example, showing access to columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// create a DataFrame using sparkSession.createDataFrame from a List or Seq\n",
    "val langPercentDF = sparkSession.createDataFrame(List((\"Scala\", 35), (\"Python\", 30), (\"R\", 15), (\"Java\", 20)))\n",
    "\n",
    "// rename the columns\n",
    "val lpDF = langPercentDF.withColumnRenamed(\"_1\", \"language\").withColumnRenamed(\"_2\", \"percent\")\n",
    "\n",
    "// order the DataFrame in descending order of percentage\n",
    "lpDF.orderBy($\"percent\".desc).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading text</h3>\n",
    "\n",
    "Aside from creating a dataset by transforming a previous one, we can also read data from a file directly into a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Read a csv file\n",
    "val dfCrime = sparkSession.read.option(\"header\",\"true\").csv(\"files/SacramentocrimeJanuary2006.csv\")\n",
    "dfCrime.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read plain text as a dataset, we need an extra <tt>import</tt> for schema conversion. Once the text is read in, operations can be carried out to find line lengths, total length of text or anything else you may want to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Read a plain text file\n",
    "import sparkSession.implicits._\n",
    "\n",
    "// class converts from dataframe to dataset output\n",
    "val bookDS = sparkSession.read.text(\"files/TaleOfTwoCities.txt\").as[String]\n",
    "bookDS.show()\n",
    "\n",
    "val lineLengths = bookDS.map(s => s.length)\n",
    "\n",
    "// To maintain lineLengths in memory\n",
    "// lineLengths.persist()\n",
    "\n",
    "val totalLength = lineLengths.reduce((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transformations</h3>\n",
    "\n",
    "We create other datasets from an existing dataset using **transformations**. A list of some of the possible transformations is available [here](http://spark.apache.org/docs/latest/programming-guide.html#transformations), and some examples follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val words = bookDS.flatMap(value => value.split(\"\\\\s+\"))\n",
    "words.show()\n",
    "val groupedWords = words.groupByKey(_.toLowerCase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Actions</h3>\n",
    "\n",
    "Some of the most common actions are available from [this page](http://spark.apache.org/docs/latest/programming-guide.html#actions). For example, <tt>count</tt> returns the number of elements in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val counts = groupedWords.count()\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pipelines</h2>\n",
    "\n",
    "It is common that a number of algorithms need to run on some data. MLlib allows this to be encoded as a [pipeline](http://spark.apache.org/docs/latest/ml-pipeline.html), and it takes care of input / output of each phase.\n",
    "\n",
    "We demonstrate a simple pipeline using the task of stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Prepare dataset consisting of (id, text) tuples.\n",
    "val dataSet = sparkSession.createDataFrame(Seq(\n",
    "  (0, \"I saw the red baloon\"),\n",
    "  (1, \"Mary had a little lamb\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "val wordsData = tokenizer.transform(dataSet)\n",
    "wordsData.select(\"words\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will have noticed in the previous notebook's exercises, the most common words in a text are often words such as *and*, *so* etc. These are not informative, and could be removed. Our pipeline is in two stages:\n",
    "\n",
    "1. tokenizer\n",
    "2. stop word removal\n",
    "\n",
    "These two stages are to be run in that order, and the input DataFrame will be transformed as it passes through them. Both stages are Transformer stages, and so the <tt>transform()</tt> method will be called on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Configure an ML pipeline, which consists of two stages: tokenizer, and stopWordsRemover.\n",
    "\n",
    "val tokenizer = new Tokenizer()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"words\")\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "    .setInputCol(\"words\")\n",
    "    .setOutputCol(\"filtered\")\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer,remover))\n",
    "\n",
    "val model = pipeline.fit(dataSet)\n",
    "val result = model.transform(dataSet)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the full power of pipelines, we present a second example: one which includes an estimator in the form of logistic regression. This pipeline has three steps:\n",
    "\n",
    "1. split each document's text into words (<i>tokenizer</i>)\n",
    "2. convert each document's words into a feature vector (<i>hashingTF</i>)\n",
    "3. learn a prediction model using the features vectors and labels (<i>logistic regression</i>)\n",
    "\n",
    "For Estimator stages, the <tt>fit()</tt> method is called to produce a Transformer and that Transformer’s <tt>transform()</tt> method is called on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Prepare training documents from a list of (id, text, label) tuples.\n",
    "val training = sparkSession.createDataFrame(Seq(\n",
    "    (0L, \"a b c d e spark\", 1.0),\n",
    "    (1L, \"b d\", 0.0),\n",
    "    (2L, \"spark f g h\", 1.0),\n",
    "    (3L, \"hadoop mapreduce\", 0.0)\n",
    ")).toDF(\"id\", \"text\", \"label\")\n",
    "\n",
    "// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "val tokenizer = new Tokenizer()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"words\")\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "    .setNumFeatures(1000)\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "    .setMaxIter(10)\n",
    "    .setRegParam(0.01)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training)\n",
    "\n",
    "// Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "val test = sparkSession.createDataFrame(Seq(\n",
    "    (4L, \"spark i j k\"),\n",
    "    (5L, \"l m n\"),\n",
    "    (6L, \"mapreduce spark\"),\n",
    "    (7L, \"apache hadoop\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "// Make predictions on test documents.\n",
    "model.transform(test)\n",
    "    .select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "    .collect()\n",
    "    .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>\n",
    "        println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercises</h2>\n",
    "\n",
    "<h3>Exercise 1</h3>\n",
    "\n",
    "In the CSV file above, <tt>[SacramentoCrime](http://samplecsvs.s3.amazonaws.com/SacramentocrimeJanuary2006.csv)</tt>, the <tt>ucr_ncic_code</tt> represents the type of crime carried out. Use any transformations / actions to output crime types in descending order of frequency. You should create this as a standalone program.\n",
    "\n",
    "<h3>Exercise 2</h3>\n",
    "\n",
    "As well as the \"[TaleOfTwoCities.txt](files/TaleOfTwoCities.txt)\", the files directory contains the file \"[GreatExpectations.txt](files/GreatExpectations.txt)\". Read in both files, and find the top 20 most frequent (overall) words that appear in both documents. (You will need to convert the documents to lower case, but you can assume that ends of line and whitespace indicate word boundaries.)\n",
    "\n",
    "<h3>Exercise 3</h3>\n",
    "\n",
    "There are a [lot of transformers and estimators](http://spark.apache.org/docs/latest/ml-features.html) implemented within Spark that can be pipelined. Create a pipeline which prints n-grams from the [TaleOfTwoCities.txt](files/TaleOfTwoCities.txt) and the [GreatExpectations.txt](files/GreatExpectations.txt) files."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
